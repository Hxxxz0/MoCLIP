{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/user/dxc/motion/StableMoFusion/'))\n",
    "from motion_loader import get_dataset_loader  \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "0.0001\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "opt = Namespace(**config)\n",
    "\n",
    "\n",
    "print(opt.batch_size)  \n",
    "print(opt.lr)          \n",
    "print(opt.device)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading train mode HumanML3D dataset ...\n",
      "11111111111111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14053c93791456db5dc8b6026e200a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing loading t2m dataset\n",
      "\n",
      " Loading gt_eval mode HumanML3D dataset ...\n",
      "11111111111111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5962cc8dd60a4e1c92140d955492d011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing loading t2m dataset\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('/home/user/dxc/motion/StableMoFusion'))\n",
    "train_loader = get_dataset_loader(\n",
    "        opt,\n",
    "        batch_size=opt.batch_size,\n",
    "        split='train',\n",
    "        mode='train'\n",
    "    )\n",
    "test_loader = get_dataset_loader(\n",
    "    opt,\n",
    "    batch_size=opt.batch_size,\n",
    "    split='test',\n",
    "    mode='gt_eval'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load clip model for stage 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_model = CLIPModel.from_pretrained(opt.clip_model_name)\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(opt.clip_model_name)\n",
    "\n",
    "# 初始阶段：冻结整个 CLIP 文本编码器（stage 1）\n",
    "for name, param in clip_model.named_parameters():\n",
    "    if \"text_model\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "motion_encoder = MotionEncoder(\n",
    "    input_dim=opt.input_dim,\n",
    "    embed_dim=opt.embed_dim,\n",
    "    num_heads=8,\n",
    "    num_layers=4,         \n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.2,\n",
    "    max_seq_length=opt.max_seq_length\n",
    ")\n",
    "model = ClipMotionAlignModel(\n",
    "    clip_model=clip_model,\n",
    "    motion_encoder=motion_encoder,\n",
    "    temperature=0.07\n",
    ").to(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=opt.lr,\n",
    "        weight_decay=opt.weight_decay\n",
    "    )\n",
    "\n",
    "best_test_loss = float(\"inf\")\n",
    "no_improve_count = 0\n",
    "max_no_improve = 3  # 连续3次验证无改进则早停"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:   0%|          | 0/767 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 767/767 [01:45<00:00,  7.30it/s, loss=1.3750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Train Average Loss: 1.8424\n",
      "[Validate at epoch 1] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_1_Test: 100%|██████████| 145/145 [00:11<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_1_Test Average Contrastive Loss: 1.5630\n",
      "Epoch_1_Test M->T Retrieval (per 32 samples): R@1=0.494, R@2=0.675, R@3=0.770\n",
      "Epoch_1_Test T->M Retrieval (per 32 samples): R@1=0.514, R@2=0.698, R@3=0.790\n",
      "Model saved: clip_motion_align_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 767/767 [01:44<00:00,  7.36it/s, loss=1.2818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300] - Train Average Loss: 1.3815\n",
      "[Validate at epoch 2] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_2_Test: 100%|██████████| 145/145 [00:11<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_2_Test Average Contrastive Loss: 1.3516\n",
      "Epoch_2_Test M->T Retrieval (per 32 samples): R@1=0.557, R@2=0.739, R@3=0.818\n",
      "Epoch_2_Test T->M Retrieval (per 32 samples): R@1=0.571, R@2=0.759, R@3=0.834\n",
      "Model saved: clip_motion_align_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 767/767 [01:45<00:00,  7.30it/s, loss=1.4553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300] - Train Average Loss: 1.2163\n",
      "[Validate at epoch 3] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_3_Test: 100%|██████████| 145/145 [00:10<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_3_Test Average Contrastive Loss: 1.2700\n",
      "Epoch_3_Test M->T Retrieval (per 32 samples): R@1=0.580, R@2=0.763, R@3=0.844\n",
      "Epoch_3_Test T->M Retrieval (per 32 samples): R@1=0.594, R@2=0.772, R@3=0.850\n",
      "Model saved: clip_motion_align_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 767/767 [01:59<00:00,  6.39it/s, loss=1.1510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300] - Train Average Loss: 1.0818\n",
      "[Validate at epoch 4] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_4_Test: 100%|██████████| 145/145 [00:10<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_4_Test Average Contrastive Loss: 1.2076\n",
      "Epoch_4_Test M->T Retrieval (per 32 samples): R@1=0.606, R@2=0.776, R@3=0.858\n",
      "Epoch_4_Test T->M Retrieval (per 32 samples): R@1=0.617, R@2=0.785, R@3=0.861\n",
      "Model saved: clip_motion_align_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 767/767 [01:46<00:00,  7.22it/s, loss=0.8676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300] - Train Average Loss: 1.0001\n",
      "[Validate at epoch 5] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_5_Test: 100%|██████████| 145/145 [00:10<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_5_Test Average Contrastive Loss: 1.1730\n",
      "Epoch_5_Test M->T Retrieval (per 32 samples): R@1=0.610, R@2=0.790, R@3=0.866\n",
      "Epoch_5_Test T->M Retrieval (per 32 samples): R@1=0.634, R@2=0.801, R@3=0.869\n",
      "Model saved: clip_motion_align_epoch_5.pt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m clip_model\u001b[38;5;241m.\u001b[39mtext_model\u001b[38;5;241m.\u001b[39mfinal_layer_norm\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      8\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_finetune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 2: Fine-tuning CLIP text encoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms last layer (and final_layer_norm) with lower lr.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/stablemofusion/lib/python3.8/site-packages/torch/optim/adamw.py:62\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m,\n\u001b[1;32m     61\u001b[0m              weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lr))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.num_epochs):\n",
    "    \n",
    "    if epoch + 1 == opt.pretrain_epochs + 1:\n",
    "        \n",
    "        for param in clip_model.text_model.encoder.layers[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in clip_model.text_model.final_layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=opt.lr_finetune,\n",
    "            weight_decay=opt.weight_decay\n",
    "        )\n",
    "        print(\"Stage 2: Fine-tuning CLIP text encoder's last layer (and final_layer_norm) with lower lr.\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{opt.num_epochs}\")\n",
    "\n",
    "\n",
    "\n",
    "    for step, batch_data in enumerate(pbar):\n",
    "        caption, motion, m_length = batch_data\n",
    "\n",
    "        \n",
    "        caption = [c.lower() for c in caption]\n",
    "        text_enc = clip_tokenizer(\n",
    "            caption,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=opt.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = text_enc[\"input_ids\"].to(opt.device)\n",
    "        attention_mask = text_enc[\"attention_mask\"].to(opt.device)\n",
    "\n",
    "       \n",
    "        if isinstance(motion, list):\n",
    "            motion = torch.stack([torch.tensor(m, dtype=torch.float32) for m in motion], dim=0)\n",
    "        else:\n",
    "            motion = motion.float()\n",
    "        motion = motion.to(opt.device)\n",
    "        m_length = m_length.to(opt.device)\n",
    "\n",
    "        \n",
    "        motion_emb, text_emb = model(motion, m_length, input_ids, attention_mask)\n",
    "        loss = clip_contrastive_loss(motion_emb, text_emb, model.logit_scale)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / max(count, 1)\n",
    "    print(f\"Epoch [{epoch+1}/{opt.num_epochs}] - Train Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "   \n",
    "    print(f\"[Validate at epoch {epoch+1}] ...\")\n",
    "    test_loss = evaluate_model(model, test_loader, clip_tokenizer, opt, desc=f\"Epoch_{epoch+1}_Test\")\n",
    "    model_path = f\"clip_motion_align_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "   \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= max_no_improve:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablemofusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
