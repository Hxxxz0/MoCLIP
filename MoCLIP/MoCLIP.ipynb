{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/user/dxc/motion/StableMoFusion/'))\n",
    "from motion_loader import get_dataset_loader  \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "0.0001\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "opt = Namespace(**config)\n",
    "\n",
    "\n",
    "print(opt.batch_size)  \n",
    "print(opt.lr)          \n",
    "print(opt.device)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading train mode HumanML3D dataset ...\n",
      "11111111111111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a528ea38104cd4b21a814b03f15396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing loading t2m dataset\n",
      "\n",
      " Loading gt_eval mode HumanML3D dataset ...\n",
      "11111111111111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aea7628fbe04c99b8983e6cfc211e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing loading t2m dataset\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('/home/user/dxc/motion/StableMoFusion'))\n",
    "train_loader = get_dataset_loader(\n",
    "        opt,\n",
    "        batch_size=opt.batch_size,\n",
    "        split='train',\n",
    "        mode='train'\n",
    "    )\n",
    "test_loader = get_dataset_loader(\n",
    "    opt,\n",
    "    batch_size=opt.batch_size,\n",
    "    split='test',\n",
    "    mode='gt_eval'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load clip model for stage 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_model = CLIPModel.from_pretrained(opt.clip_model_name)\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(opt.clip_model_name)\n",
    "\n",
    "# 初始阶段：冻结整个 CLIP 文本编码器（stage 1）\n",
    "for name, param in clip_model.named_parameters():\n",
    "    if \"text_model\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "motion_encoder = MotionEncoder(\n",
    "    input_dim=opt.input_dim,\n",
    "    embed_dim=opt.embed_dim,\n",
    "    num_heads=8,\n",
    "    num_layers=4,         \n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.2,\n",
    "    max_seq_length=opt.max_seq_length\n",
    ")\n",
    "model = ClipMotionAlignModel(\n",
    "    clip_model=clip_model,\n",
    "    motion_encoder=motion_encoder,\n",
    "    temperature=0.07\n",
    ").to(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=opt.lr,\n",
    "        weight_decay=opt.weight_decay\n",
    "    )\n",
    "\n",
    "best_test_loss = float(\"inf\")\n",
    "no_improve_count = 0\n",
    "max_no_improve = 3  # 连续3次验证无改进则早停"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:   0%|          | 0/767 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 767/767 [01:47<00:00,  7.11it/s, loss=1.3735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Train Average Loss: 1.8478\n",
      "[Validate at epoch 1] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_1_Test: 100%|██████████| 145/145 [00:13<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_1_Test Average Contrastive Loss: 1.5263\n",
      "Epoch_1_Test M->T Retrieval (per 32 samples): R@1=0.502, R@2=0.681, R@3=0.780\n",
      "Epoch_1_Test T->M Retrieval (per 32 samples): R@1=0.526, R@2=0.714, R@3=0.798\n",
      "Model saved: clip_motion_align_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 767/767 [01:40<00:00,  7.64it/s, loss=1.1631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300] - Train Average Loss: 1.3659\n",
      "[Validate at epoch 2] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_2_Test: 100%|██████████| 145/145 [00:10<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_2_Test Average Contrastive Loss: 1.3374\n",
      "Epoch_2_Test M->T Retrieval (per 32 samples): R@1=0.553, R@2=0.739, R@3=0.827\n",
      "Epoch_2_Test T->M Retrieval (per 32 samples): R@1=0.583, R@2=0.756, R@3=0.834\n",
      "Model saved: clip_motion_align_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 767/767 [01:42<00:00,  7.51it/s, loss=0.9696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300] - Train Average Loss: 1.1982\n",
      "[Validate at epoch 3] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_3_Test: 100%|██████████| 145/145 [00:10<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_3_Test Average Contrastive Loss: 1.2602\n",
      "Epoch_3_Test M->T Retrieval (per 32 samples): R@1=0.578, R@2=0.764, R@3=0.839\n",
      "Epoch_3_Test T->M Retrieval (per 32 samples): R@1=0.599, R@2=0.772, R@3=0.847\n",
      "Model saved: clip_motion_align_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 767/767 [02:06<00:00,  6.04it/s, loss=0.9263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300] - Train Average Loss: 1.0762\n",
      "[Validate at epoch 4] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_4_Test: 100%|██████████| 145/145 [00:10<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_4_Test Average Contrastive Loss: 1.2273\n",
      "Epoch_4_Test M->T Retrieval (per 32 samples): R@1=0.598, R@2=0.784, R@3=0.861\n",
      "Epoch_4_Test T->M Retrieval (per 32 samples): R@1=0.615, R@2=0.787, R@3=0.865\n",
      "Model saved: clip_motion_align_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 767/767 [01:37<00:00,  7.90it/s, loss=1.2109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300] - Train Average Loss: 1.0004\n",
      "[Validate at epoch 5] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_5_Test: 100%|██████████| 145/145 [00:12<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_5_Test Average Contrastive Loss: 1.1726\n",
      "Epoch_5_Test M->T Retrieval (per 32 samples): R@1=0.619, R@2=0.791, R@3=0.864\n",
      "Epoch_5_Test T->M Retrieval (per 32 samples): R@1=0.630, R@2=0.801, R@3=0.866\n",
      "Model saved: clip_motion_align_epoch_5.pt\n",
      "Stage 2: Fine-tuning CLIP text encoder's last layer (and final_layer_norm) with lower lr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████| 767/767 [01:44<00:00,  7.37it/s, loss=0.6682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300] - Train Average Loss: 0.8040\n",
      "[Validate at epoch 6] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_6_Test: 100%|██████████| 145/145 [00:10<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_6_Test Average Contrastive Loss: 1.0273\n",
      "Epoch_6_Test M->T Retrieval (per 32 samples): R@1=0.659, R@2=0.825, R@3=0.895\n",
      "Epoch_6_Test T->M Retrieval (per 32 samples): R@1=0.664, R@2=0.828, R@3=0.894\n",
      "Model saved: clip_motion_align_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████| 767/767 [01:44<00:00,  7.34it/s, loss=1.1500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300] - Train Average Loss: 0.7119\n",
      "[Validate at epoch 7] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_7_Test: 100%|██████████| 145/145 [00:20<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_7_Test Average Contrastive Loss: 1.0261\n",
      "Epoch_7_Test M->T Retrieval (per 32 samples): R@1=0.664, R@2=0.828, R@3=0.895\n",
      "Epoch_7_Test T->M Retrieval (per 32 samples): R@1=0.674, R@2=0.836, R@3=0.896\n",
      "Model saved: clip_motion_align_epoch_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████| 767/767 [01:51<00:00,  6.90it/s, loss=0.6629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300] - Train Average Loss: 0.6702\n",
      "[Validate at epoch 8] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_8_Test: 100%|██████████| 145/145 [00:10<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_8_Test Average Contrastive Loss: 1.0127\n",
      "Epoch_8_Test M->T Retrieval (per 32 samples): R@1=0.669, R@2=0.826, R@3=0.896\n",
      "Epoch_8_Test T->M Retrieval (per 32 samples): R@1=0.680, R@2=0.836, R@3=0.899\n",
      "Model saved: clip_motion_align_epoch_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████| 767/767 [01:42<00:00,  7.48it/s, loss=0.3763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300] - Train Average Loss: 0.6254\n",
      "[Validate at epoch 9] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_9_Test: 100%|██████████| 145/145 [00:10<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_9_Test Average Contrastive Loss: 0.9889\n",
      "Epoch_9_Test M->T Retrieval (per 32 samples): R@1=0.676, R@2=0.837, R@3=0.895\n",
      "Epoch_9_Test T->M Retrieval (per 32 samples): R@1=0.691, R@2=0.850, R@3=0.901\n",
      "Model saved: clip_motion_align_epoch_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|██████████| 767/767 [01:47<00:00,  7.14it/s, loss=0.5926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300] - Train Average Loss: 0.5808\n",
      "[Validate at epoch 10] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_10_Test: 100%|██████████| 145/145 [00:10<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_10_Test Average Contrastive Loss: 0.9776\n",
      "Epoch_10_Test M->T Retrieval (per 32 samples): R@1=0.675, R@2=0.843, R@3=0.902\n",
      "Epoch_10_Test T->M Retrieval (per 32 samples): R@1=0.686, R@2=0.844, R@3=0.905\n",
      "Model saved: clip_motion_align_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 767/767 [01:48<00:00,  7.06it/s, loss=0.5308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300] - Train Average Loss: 0.5597\n",
      "[Validate at epoch 11] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_11_Test: 100%|██████████| 145/145 [00:13<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_11_Test Average Contrastive Loss: 0.9677\n",
      "Epoch_11_Test M->T Retrieval (per 32 samples): R@1=0.686, R@2=0.849, R@3=0.906\n",
      "Epoch_11_Test T->M Retrieval (per 32 samples): R@1=0.684, R@2=0.850, R@3=0.908\n",
      "Model saved: clip_motion_align_epoch_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 767/767 [01:41<00:00,  7.57it/s, loss=0.6088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300] - Train Average Loss: 0.5258\n",
      "[Validate at epoch 12] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_12_Test: 100%|██████████| 145/145 [00:10<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_12_Test Average Contrastive Loss: 0.9872\n",
      "Epoch_12_Test M->T Retrieval (per 32 samples): R@1=0.685, R@2=0.842, R@3=0.900\n",
      "Epoch_12_Test T->M Retrieval (per 32 samples): R@1=0.693, R@2=0.845, R@3=0.905\n",
      "Model saved: clip_motion_align_epoch_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|██████████| 767/767 [01:40<00:00,  7.65it/s, loss=0.5552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300] - Train Average Loss: 0.4936\n",
      "[Validate at epoch 13] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_13_Test: 100%|██████████| 145/145 [00:09<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_13_Test Average Contrastive Loss: 0.9213\n",
      "Epoch_13_Test M->T Retrieval (per 32 samples): R@1=0.693, R@2=0.858, R@3=0.913\n",
      "Epoch_13_Test T->M Retrieval (per 32 samples): R@1=0.708, R@2=0.859, R@3=0.912\n",
      "Model saved: clip_motion_align_epoch_13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|██████████| 767/767 [01:47<00:00,  7.13it/s, loss=0.5086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300] - Train Average Loss: 0.4660\n",
      "[Validate at epoch 14] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_14_Test: 100%|██████████| 145/145 [00:10<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_14_Test Average Contrastive Loss: 0.9463\n",
      "Epoch_14_Test M->T Retrieval (per 32 samples): R@1=0.690, R@2=0.851, R@3=0.908\n",
      "Epoch_14_Test T->M Retrieval (per 32 samples): R@1=0.707, R@2=0.852, R@3=0.908\n",
      "Model saved: clip_motion_align_epoch_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300: 100%|██████████| 767/767 [01:38<00:00,  7.76it/s, loss=0.4169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300] - Train Average Loss: 0.4450\n",
      "[Validate at epoch 15] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_15_Test: 100%|██████████| 145/145 [00:12<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_15_Test Average Contrastive Loss: 0.9549\n",
      "Epoch_15_Test M->T Retrieval (per 32 samples): R@1=0.687, R@2=0.845, R@3=0.907\n",
      "Epoch_15_Test T->M Retrieval (per 32 samples): R@1=0.700, R@2=0.852, R@3=0.905\n",
      "Model saved: clip_motion_align_epoch_15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/300: 100%|██████████| 767/767 [01:39<00:00,  7.68it/s, loss=0.4101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300] - Train Average Loss: 0.4231\n",
      "[Validate at epoch 16] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_16_Test: 100%|██████████| 145/145 [00:12<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_16_Test Average Contrastive Loss: 0.9447\n",
      "Epoch_16_Test M->T Retrieval (per 32 samples): R@1=0.703, R@2=0.857, R@3=0.913\n",
      "Epoch_16_Test T->M Retrieval (per 32 samples): R@1=0.705, R@2=0.856, R@3=0.913\n",
      "Model saved: clip_motion_align_epoch_16.pt\n",
      "Early stopping triggered!\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.num_epochs):\n",
    "    \n",
    "    if epoch + 1 == opt.pretrain_epochs + 1:\n",
    "        \n",
    "        for param in clip_model.text_model.encoder.layers[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in clip_model.text_model.final_layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=opt.lr_finetune,\n",
    "            weight_decay=opt.weight_decay\n",
    "        )\n",
    "        print(\"Stage 2: Fine-tuning CLIP text encoder's last layer (and final_layer_norm) with lower lr.\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{opt.num_epochs}\")\n",
    "\n",
    "\n",
    "\n",
    "    for step, batch_data in enumerate(pbar):\n",
    "        caption, motion, m_length = batch_data\n",
    "\n",
    "        \n",
    "        caption = [c.lower() for c in caption]\n",
    "        text_enc = clip_tokenizer(\n",
    "            caption,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=opt.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = text_enc[\"input_ids\"].to(opt.device)\n",
    "        attention_mask = text_enc[\"attention_mask\"].to(opt.device)\n",
    "\n",
    "       \n",
    "        if isinstance(motion, list):\n",
    "            motion = torch.stack([torch.tensor(m, dtype=torch.float32) for m in motion], dim=0)\n",
    "        else:\n",
    "            motion = motion.float()\n",
    "        motion = motion.to(opt.device)\n",
    "        m_length = m_length.to(opt.device)\n",
    "\n",
    "        \n",
    "        motion_emb, text_emb = model(motion, m_length, input_ids, attention_mask)\n",
    "        loss = clip_contrastive_loss(motion_emb, text_emb, model.logit_scale)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / max(count, 1)\n",
    "    print(f\"Epoch [{epoch+1}/{opt.num_epochs}] - Train Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "   \n",
    "    print(f\"[Validate at epoch {epoch+1}] ...\")\n",
    "    test_loss = evaluate_model(model, test_loader, clip_tokenizer, opt, desc=f\"Epoch_{epoch+1}_Test\")\n",
    "    model_path = f\"clip_motion_align_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "   \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= max_no_improve:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablemofusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
